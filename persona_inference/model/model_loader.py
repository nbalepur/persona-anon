from abc import ABC, abstractmethod
from enums import ModelType
import time
import openai
import anthropic
import cohere
import torch
from transformers import AutoTokenizer, pipeline
from huggingface_hub import HfFolder

# Abstract base class for implementing zero-shot prompts
class LLM(ABC):

    def __init__(self):
        pass

    @abstractmethod
    def generate_text(self, prompt: str) -> str:
        """Generate text from a prompt"""
        pass

class HuggingFaceChatModel(LLM):

    def __init__(self, hf_model_name: str, temp: float, min_length: int, max_length: int, load_in_4bit: bool, load_in_8bit: bool, device_map: str, cache_dir: str, hf_token: str):

        self.temp = temp
        self.min_length = min_length
        self.max_length = max_length

        HfFolder.save_token(hf_token)

        # load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(hf_model_name, cache_dir=cache_dir)
        dtype = {
            "bf16": torch.bfloat16,
            "fp32": torch.float32,
            "fp16": torch.float16,
            "auto": "auto",
        }['auto']
        pipe = pipeline(
            'text-generation',
            model=hf_model_name,
            tokenizer=tokenizer,
            device_map=device_map,
            torch_dtype=dtype,
            min_new_tokens=min_length,
            max_new_tokens=max_length,
            model_kwargs={"cache_dir": cache_dir, "do_sample": False, "load_in_4bit": load_in_4bit, "load_in_8bit": load_in_8bit}
        )
        self.pipe = pipe
        self.tokenizer = tokenizer

    def generate_text(self, prompt: str) -> str | None:

        messages = [{"role": "user", "content": prompt}]

        if self.temp == 0.0:
            return self.pipe(messages, 
            do_sample=False,  
            min_new_tokens=self.min_length, 
            max_new_tokens=self.max_length,
            return_full_text=False)[0]['generated_text'].strip()
        else:
            return self.pipe(messages, 
            do_sample=True, 
            temperature=self.temp, 
            min_new_tokens=self.min_length, 
            max_new_tokens=self.max_length,
            return_full_text=False)[0]['generated_text'].strip()

class OpenAI(LLM):

    def __init__(self, openai_model_name: str, temp: float, max_length: int, stop_token: str, openai_token: str):

        self.temp = temp
        self.openai_model_name = openai_model_name
        self.max_length = max_length
        self.stop_token = stop_token
        self.openai_token = openai_token

    def generate_text_helper(self, prompt: str, num_sec=0, max_retries=5) -> str | None:

        if num_sec == max_retries:
            print("MAX RETRIES EXCEDED")
            return None

        time.sleep(2**(num_sec - 1))

        try:
            client = openai.OpenAI(api_key=self.openai_token)
            response = client.chat.completions.create(
                        model=self.openai_model_name,
                        messages=[
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=self.max_length,
                        stop=[self.stop_token],
                        temperature=self.temp)
            return response.choices[0].message.content

        except Exception as e:
            return self.generate_text_helper(prompt, num_sec=num_sec+1, max_retries=max_retries)

    def generate_text(self, prompt) -> str | None:
        
        return self.generate_text_helper(prompt, num_sec=0, max_retries=3)

class Cohere(LLM):

    def __init__(self, cohere_model_name: str, temp: float, max_length: int, stop_token: str, cohere_token: str):

        self.temp = temp
        self.cohere_model_name = cohere_model_name
        self.max_length = max_length
        self.stop_token = stop_token
        self.co = cohere.Client(cohere_token)

    def generate_text_helper(self, prompt, num_sec=0, max_retries=5):

        if num_sec == max_retries:
            print("MAX RETRIES EXCEDED")
            return None

        time.sleep(2**(num_sec))

        try:
            response = self.co.chat(
                message=prompt,
                max_tokens=self.max_length,
                model=self.cohere_model_name,
                stop_sequences=[self.stop_token],
                temperature=self.temp,
            )
            return response.text

        except Exception as e:
            return self.generate_text_helper(prompt, num_sec=num_sec+1, max_retries=max_retries)

    def generate_text(self, prompt):
        return self.generate_text_helper(prompt, num_sec=0, max_retries=3)

class Anthropic(LLM):

    def __init__(self, model_name, temp, max_length, stop_token, anthropic_token):

        self.temp = temp
        self.model_name = model_name
        self.max_length = max_length
        self.stop_token = stop_token
        self.client = anthropic.Anthropic(api_key=anthropic_token)

    def generate_text_helper(self, prompt, num_sec=0, max_retries=5):

        if num_sec == max_retries:
            print("MAX RETRIES EXCEDED")
            return None

        time.sleep(2**(num_sec))

        try:
            message = self.client.messages.create(
                model=self.model_name,
                max_tokens=self.max_length,
                temperature=self.temp,
                stop_sequences=[self.stop_token],
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            }
                        ]
                    }
                ]
            )
            return message.content[0].text

        except Exception as e:
            return self.generate_text_helper(prompt, num_sec=num_sec+1, max_retries=max_retries)

    def generate_text(self, prompt):
        return self.generate_text_helper(prompt, num_sec=0, max_retries=3)

class ModelFactory:

    @staticmethod
    def get_model(args):
        if args.model_type[0] == ModelType.hf_chat:
            return HuggingFaceChatModel(hf_model_name=args.model_name, temp=args.temperature, min_length=args.min_tokens, max_length=args.max_tokens, load_in_4bit=args.load_in_4bit, load_in_8bit=args.load_in_8bit, device_map=args.device_map, cache_dir=args.cache_dir, hf_token=args.hf_token)
        
        if args.model_type[0] == ModelType.open_ai:
            return OpenAI(openai_model_name=args.model_name, temp=args.temperature, max_length=args.max_tokens, stop_token=args.stop_token, openai_token=args.open_ai_token)

        if args.model_type[0] == ModelType.cohere:
            return Cohere(cohere_model_name=args.model_name, temp=args.temperature, max_length=args.max_tokens, stop_token=args.stop_token, cohere_token=args.cohere_token)

        if args.model_type[0] == ModelType.anthropic:
            return Anthropic(model_name=args.model_name, temp=args.temperature, max_length=args.max_tokens, stop_token=args.stop_token, anthropic_token=args.anthropic_token)

        else:
            raise ValueError(f"Unsupported Model type: {args.model_type[0]}")
